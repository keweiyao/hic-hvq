#!/usr/bin/env python3

from itertools import chain
import sys
import subprocess
from multiprocessing import Pool, cpu_count

import numpy as np
import h5py, math

import freestream
import frzout
import fonll.fonll as fonll
import JetCalc.LeadingParton as JLP
from JetCalc.ExpCut import cuts as JEC

def afterburner(Nstart, Nstop):
	for index in range(Nstart, Nstop):
		run_cmd('run_urqmd urqmd_input_{:03d} urqmd_output_{:03d}'.format(index, index))


def run_cmd(*args, **kwargs):
	"""
	Run a subprocess, concatenating argument strings together.

	"""
	print(*args, flush=True)  # flush stdout to retain output order
	subprocess.check_call(
		list(chain.from_iterable(a.split() for a in args)),
		**kwargs
	)


def read_text_file(filename):
	"""
	Read a text file into a nested list of bytes objects,
	skipping comment lines (#).

	"""
	with open(filename, 'rb') as f:
		return [l.split() for l in f if not l.startswith(b'#')]

def save_fs_history(ic, event_size, grid_step, tau_fs, xi, grid_max, steps=5, coarse=False):
	f = h5py.File('FreeStream.h5')
	dxy = grid_step*(coarse or 1)
	ls = math.ceil(event_size/dxy)
	n = 2*ls + 1
	NX, NY = ic.shape
	# roll ic by index 1 to match hydro
	ic = np.roll(np.roll(ic, shift=-1, axis=0), shift=-1, axis=1)
	tau0 = tau_fs*xi
	taus = np.linspace(tau0, tau_fs, steps)
	dtau = taus[1]-taus[0]
	gp = f.create_group('Event')
	gp.attrs.create('XL', [-ls])
	gp.attrs.create('XH', [ls])
	gp.attrs.create('YL', [-ls])
	gp.attrs.create('YH', [ls])
	gp.attrs.create('Tau0', [tau0])
	gp.attrs.create('dTau', [dtau]) 
	gp.attrs.create('DX', [dxy])
	gp.attrs.create('DY', [dxy])
	gp.attrs.create('NTau', [steps])
	for itau, tau in enumerate(taus):
		print(tau)
		frame = gp.create_group('Frame_{:04d}'.format(itau))
		fs = freestream.FreeStreamer(ic, grid_max, tau)
		for fmt, data, arglist in [
			('e', fs.energy_density, [()]),
			('V{}', fs.flow_velocity, [(1,), (2,)]),
			('Pi{}{}', fs.shear_tensor, [(0,0), (0,1), (0,2),
												(1,1), (1,2),
													   (2,2)] ),
			]:
			for a in arglist:
				X = data(*a).T # to get the correct x-y with vishnew
				if fmt == 'V{}':
					X = X/data(0).T
				if coarse:
					X = X[::coarse, ::coarse]
				diff = X.shape[0] - n
				start = int(abs(diff)/2)

				if diff > 0:
					# original grid is larger -> cut out middle square
					s = slice(start, start + n)
					X = X[s, s]
				elif diff < 0:
					# original grid is smaller
					#  -> create new array and place original grid in middle
					Xn = np.zeros((n, n))
					s = slice(start, start + X.shape[0])
					Xn[s, s] = X
					X = Xn
				if fmt == 'V{}':
					Comp = {1:'x', 2:'y'}
					frame.create_dataset(fmt.format(Comp[a[0]]), data=X)
				if fmt == 'e':
					frame.create_dataset(fmt.format(*a), data=X)
					frame.create_dataset('P', data=X/3.)
					frame.create_dataset('BulkPi', data=X*0.)
					prefactor = 1.0/15.62687/5.068**3 
					frame.create_dataset('Temp', data=(X*prefactor)**0.25)
					s = (X + frame['P'].value)/(frame['Temp'].value+1e-14)
					frame.create_dataset('s', data=s)
				if fmt == 'Pi{}{}': 
					frame.create_dataset(fmt.format(*a), data=X)
		pi33 = -(frame['Pi00'].value + frame['Pi11'].value \
									 + frame['Pi22'].value)
		frame.create_dataset('Pi33', data=pi33)
		pi3Z = np.zeros_like(pi33)
		frame.create_dataset('Pi03', data=pi3Z)
		frame.create_dataset('Pi13', data=pi3Z)
		frame.create_dataset('Pi23', data=pi3Z)
	f.close()


def main():
	fresult = h5py.File('results.hdf5','a')
	# parse config file
	if len(sys.argv) >= 2:
		with open(sys.argv[1], 'r') as f:
			config = dict(
				(i.strip() for i in l.split('=', maxsplit=1))
				for l in f if l[0] != '#'
			)
		if len(sys.argv) == 3:
			run_id = int(sys.argv[2])
		else:
			print("Hi")
			run_id = 0
	else:
		config = {}
	print ('==========',run_id, '=============')
	nevents = 1
	grid_step = 0.1
	grid_max = 15.05
	proj, targ = 'Pb', 'Pb'
	system = proj+targ
	# run trento and yield initial entropy density arrays
	def initial_conditions(initial_file='initial.hdf'):
		run_cmd(
			'trento {} {}'.format(proj, targ), str(nevents),
			'--grid-step {} --grid-max {}'.format(grid_step, grid_max),
			'--output', initial_file,
			config.get('trento_args', '')
		)
		with h5py.File(initial_file, 'r') as f:
			for dset in f.values():
				yield [dset['matter_density'].value, dset['Ncoll_density'].value]

	# energy
	sqrts = int(config.get('sqrts', '5020'))
	print (proj, targ, '@', sqrts, " GeV")
	
	# probe
	HQ_type = config.get('particle_type')
	print ('Studying {} quark'.format(HQ_type))
	
	# read free streaming time and enable for time > epsilon
	tau_fs = float(config.get('tau_fs', 0))
	enable_fs = tau_fs > 1e-6
	xi_fs = float(config.get('xi_fs', 0.5))
	# create sampler HRG object (to be reused for all events)
	Tswitch = float(config.get('Tswitch', .154))
	hrg = frzout.HRG(Tswitch, species='urqmd', res_width=True)
	eswitch = hrg.energy_density()

	# append switching energy density to vishnew arguments
	vishnew_args = [
		config.get('vishnew_args', ''),
		'initialuread=1 iein=0 t0={}'.format(tau_fs) if enable_fs else
		'initialuread=0 iein=1',
		'edec={}'.format(eswitch)
	]

	# Heavy quark species (name, ID)
	mass = {'c': 1.3, 'b': 4.2}[HQ_type]
	HM_species = { 'c':
		[('D+', 411), ('D0', 421),
		 ('D*+', 10411), ('D0*', 10421),
		 ('Ds+', 431), ('Ds*+', 433)],
		'b':
		[('B0', 511), ('B+',521),
		 ('B*0', 513), ('B*+', 523),
		 ('Bs0', 531), ('Bs*0',541)] } [HQ_type]
	HM_pid = [s[1] for s in HM_species]

	# run each event
	for ievent, ic in enumerate(initial_conditions()):
		event_gp = fresult.create_group('event_{}_{}'.format(run_id, ievent))
		event_gp.attrs.create('initial_entropy', grid_step**2 * ic[0].sum())
		event_gp.attrs.create('N_coll', grid_step**2 * ic[1].sum())

		# ==================IC+freestream===========================
		if enable_fs:
			# save multi frame of freestreaming energy density 
			# from xi*tau_fs to tau_fs, with 0.1 < xi < 1.0
			# free stream initial condition
			save_fs_history(ic[0], event_size=grid_max, grid_step=grid_step, 
							tau_fs=tau_fs, xi=xi_fs, steps=10, grid_max=grid_max, coarse=2)
			fs = freestream.FreeStreamer(ic[0], grid_max, tau_fs)
			e = fs.energy_density()
			e_above = e[e > eswitch].sum()
			event_gp.attrs.create('mult_factor', e.sum()/e_above if e_above > 0 else 1)

			np.savetxt('ed.dat', e)
			for i in [1, 2]:
				np.savetxt('u{}.dat'.format(i), fs.flow_velocity(i))
			for ij in [(1, 1), (1, 2), (2, 2)]:
				np.savetxt(
					'pi{}{}.dat'.format(*ij), fs.shear_tensor(*ij))
		else:
			# skip free streaming, use initial condition as entropy density
			np.savetxt('sd.dat', ic[0])
		# ==================Vishnew===========================
		# hydro
		run_cmd('vishnew', *vishnew_args)
		# ==================Frzout Sampler===========================
		# read freeze-out surface data
		surface_data = np.array(
			read_text_file('surface.dat'),
			dtype=float
		)
		# end event if the surface is empty -- this occurs in ultra-peripheral
		# events where the initial condition doesn't exceed Tswitch
		if surface_data.size == 0:
			print('empty hypersurface')
			continue

		# unpack surface_data columns:
		#   0	1  2  3		 4		 5		 6	7
		#   tau  x  y  dsigma^t  dsigma^x  dsigma^y  v_x  v_y
		#   8	 9	 10	11	12	13	14	15
		#   pitt  pitx  pity  pixx  pixy  piyy  pizz  Pi
		x, sigma, v, _ = np.hsplit(surface_data, [3, 6, 8])
		pi = dict(zip(['xx', 'xy', 'yy'], surface_data.T[11:14]))
		Pi = surface_data.T[15]
		# create sampler surface object
		surface = frzout.Surface(x, sigma, v, pi=pi, Pi=Pi, ymax=3.)

		minsamples, maxsamples = 10, 100  # reasonable range for nsamples
		minparts = 30000  # min number of particles to sample
		nparts = 0  # for tracking total number of sampled particles
		# sample soft particles and write to file
		with open('particles_in.dat', 'w') as f:
			for nsamples in range(1, maxsamples + 1):
				parts = frzout.sample(surface, hrg)
				if parts.size == 0:
					continue
				nparts += parts.size
				print('#', parts.size, file=f)
				for p in parts:
					print(p['ID'], *chain(p['x'], p['p']), file=f)
				if nparts >= minparts and nsamples >= minsamples:
					break

		if nparts == 0:
			print('no particles produced')
			continue

		event_gp.attrs.create('nsamples', nsamples, dtype=np.int)

		# ==================Heavy Flavor===========================
		run_cmd('run-hvq-events {} {}'.format(sys.argv[1], run_id))
				
		# ==================Heavy + Soft --> UrQMD===========================
		# step 1: combine soft and hard particles and split into n-oversamples
		run_cmd('convert_format {} particles_in.dat h-meson-final-{}.dat'.format(nsamples, run_id, run_id))
		# step 2: split these n-oversamples into separate files
		subprocess.call('csplit --elide-empty-files --digits=3 --quiet --prefix=urqmd_input_ urqmd_input.dat "/UQMD/" "{*}" ', shell=True)
		# step 3: parallely run each oversamples:
		Nproc = np.min([cpu_count()-1 or 1, 10, nsamples])
		Nstep = np.floor(nsamples*1./Nproc).astype(int)
		Nstart = (np.array(range(Nproc))*Nstep).astype(int)
		Nstop = Nstart + Nstep
		Nstop[-1] = nsamples
		print("# start index", Nstart)
		print("# stop index", Nstop)
		with Pool(Nproc) as pool:
			pool.starmap(afterburner, zip(Nstart, Nstop))
		# step 4: combine all outputs together
		subprocess.call('cat urqmd_output_* > particles_out_{}.dat'.format(run_id), shell=True)
		# step 5: clean up
		subprocess.call('rm -v urqmd_output_* urqmd_input_* urqmd_input.dat', shell=True)
				
		# ==================Data Processing==================================
				# load fonll 
		fonllcalc = fonll.FONLL()

		# read final particle data
		ID, charge, fmass, px, py, pz, y, eta, pT0, y0, s1, s2 = (
			np.array(col, dtype=dtype) for (col, dtype) in
			zip(
				zip(*read_text_file('particles_out_{}.dat'.format(run_id))),
				(2*[int] + 10*[float])
			)
		)

		# pT, phi, and id cut
		pT = np.sqrt(px**2+py**2)
		phi = np.arctan2(py, px)
		charged = (charge != 0)
		abs_eta = np.fabs(eta)
		abs_ID = np.abs(ID)
		is_heavy = np.array([iid in HM_pid for iid in abs_ID], dtype=bool)
		is_light = np.logical_not(is_heavy)

		#============for soft particles======================
		event_gp.attrs.create('dNch_deta',
								np.count_nonzero(charged & (abs_eta<.5) & is_light) / nsamples)
		for Collab in ['ALICE', 'CMS']: 
			ref_gp = event_gp.create_group(Collab+"/REF")
			poi_gp = event_gp.create_group(Collab+"/POI")
			
			#=========Event plane Q-vector from UrQMD events======================
			phi_light = phi[charged & is_light \
				& (JEC[Collab]['vn_ref']['ybins'][0] < eta) \
				& (eta < JEC[Collab]['vn_ref']['ybins'][1]) \
				& (JEC[Collab]['vn_ref']['pTbins'][0] < pT) \
				& (pT < JEC[Collab]['vn_ref']['pTbins'][1])]
			ref_gp.create_dataset('M', data=phi_light.shape[0], dtype=np.int)
			ref_gp.create_dataset('Qn', data=\
				np.array([np.exp(1j*n*phi_light).sum() for n in range(1, 7)]), 
					dtype=np.complex)

			#============higher accuracy event-plane=============
			# oversample to get a high percision event plane at freezeout
			opTs = []; oys = []; ophis = []; counts=0; nloop=0
			while counts < 10**6 and nloop < 1000:
				nloop += 1
				oE, opx, opy, opz = frzout.sample(surface, hrg)['p'].T
				oM, opT, oy, ophi = JLP.fourvec_to_curvelinear(opx, opy, opz, oE)
				counts += len(oM)
				opTs = np.concatenate([opTs, opT])
				ophis = np.concatenate([ophis, ophi])
				oys = np.concatenate([oys, oy])
			ophi_light = ophis[(JEC[Collab]['vn_ref']['ybins'][0] < oys) \
								& (oys < JEC[Collab]['vn_ref']['ybins'][1]) \
								& (JEC[Collab]['vn_ref']['pTbins'][0] < opTs) \
								& (opTs < JEC[Collab]['vn_ref']['pTbins'][1])]
			ref_gp.create_dataset('hp-M', data=ophi_light.shape[0], dtype=np.int)
			ref_gp.create_dataset('hp-Qn', data=np.array([np.exp(1j*n*ophi_light).sum() 
								for n in range(1, 7)]), dtype=np.complex)

			#===========For heave particles======================
			# HF after Urqmd
			HF_dict = { 'pid': abs_ID[is_heavy],
						'pT' : pT[is_heavy],
						'y'  : y[is_heavy],
						'phi': phi[is_heavy],
						'w' : None
				  }
			POI = {'c': [411, 421, 10411], 'b': [511, 521] }[HQ_type]
			for nPDF in ['EPPS', 'nCTEQ']:
				w = fonllcalc.interp(nPDF, system, str(sqrts), 
					HQ_type, pT0[is_heavy], y0[is_heavy])
				HF_dict['w'] = w[:,0]
				flow = JLP.Qvector(HF_dict, JEC[Collab]['vn_HF']['pTbins'], 
									[JEC[Collab]['vn_HF']['ybins']], POI)
				Yield = JLP.Yield(HF_dict, JEC[Collab]['Raa']['pTbins'], 
									[JEC[Collab]['Raa']['ybins']], POI)
				gp = poi_gp.create_group(nPDF)
				for iid in POI:
					name = str(iid)
					gp.create_group('Yield/'+name)
					gp.create_group('vn/'+name)
					gp['Yield/'+name].create_dataset('shape', data=Yield[iid]['shape'])
					gp['Yield/'+name].create_dataset('Ntot', data=Yield[iid]['Ntot'])
					gp['vn/'+name].create_dataset('Qn', data=flow[iid]['Qn'])
					gp['vn/'+name].create_dataset('M', data=flow[iid]['M'])


	fresult.close()


if __name__ == "__main__":
	main()
