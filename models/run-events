#!/usr/bin/env python3

from itertools import chain
import sys, subprocess

import numpy as np
import h5py, math

import freestream
import frzout
import JetCalc.LeadingParton as JLP
from JetCalc.ExpCut import cuts as JEC

def run_cmd(*args, **kwargs):
	"""
	Run a subprocess, concatenating argument strings together.

	"""
	print(*args, flush=True)  # flush stdout to retain output order
	subprocess.check_call(
		list(chain.from_iterable(a.split() for a in args)),
		**kwargs
	)


def read_text_file(filename):
	"""
	Read a text file into a nested list of bytes objects,
	skipping comment lines (#).

	"""
	with open(filename, 'rb') as f:
		return [l.split() for l in f if not l.startswith(b'#')]

def save_fs_history(ic, event_size, grid_step, tau_fs, xi, grid_max, steps=5, coarse=False):
	f = h5py.File('FreeStream.h5', 'w')
	dxy = grid_step*(coarse or 1)
	ls = math.ceil(event_size/dxy)
	n = 2*ls + 1
	NX, NY = ic.shape
	# roll ic by index 1 to match hydro
	ic = np.roll(np.roll(ic, shift=-1, axis=0), shift=-1, axis=1)
	tau0 = tau_fs*xi
	taus = np.linspace(tau0, tau_fs, steps)
	dtau = taus[1]-taus[0]
	gp = f.create_group('Event')
	gp.attrs.create('XL', [-ls])
	gp.attrs.create('XH', [ls])
	gp.attrs.create('YL', [-ls])
	gp.attrs.create('YH', [ls])
	gp.attrs.create('Tau0', [tau0])
	gp.attrs.create('dTau', [dtau])
	gp.attrs.create('DX', [dxy])
	gp.attrs.create('DY', [dxy])
	gp.attrs.create('NTau', [steps])
	for itau, tau in enumerate(taus):
		print(tau)
		frame = gp.create_group('Frame_{:04d}'.format(itau))
		fs = freestream.FreeStreamer(ic, grid_max, tau)
		for fmt, data, arglist in [
			('e', fs.energy_density, [()]),
			('V{}', fs.flow_velocity, [(1,), (2,)]),
			('Pi{}{}', fs.shear_tensor, [(0,0), (0,1), (0,2),
												(1,1), (1,2),
													   (2,2)] ),
			]:
			for a in arglist:
				X = data(*a).T # to get the correct x-y with vishnew
				if fmt == 'V{}':
					X = X/data(0).T
				if coarse:
					X = X[::coarse, ::coarse]
				diff = X.shape[0] - n
				start = int(abs(diff)/2)

				if diff > 0:
					# original grid is larger -> cut out middle square
					s = slice(start, start + n)
					X = X[s, s]
				elif diff < 0:
					# original grid is smaller
					#  -> create new array and place original grid in middle
					Xn = np.zeros((n, n))
					s = slice(start, start + X.shape[0])
					Xn[s, s] = X
					X = Xn
				if fmt == 'V{}':
					Comp = {1:'x', 2:'y'}
					frame.create_dataset(fmt.format(Comp[a[0]]), data=X)
				if fmt == 'e':
					frame.create_dataset(fmt.format(*a), data=X)
					frame.create_dataset('P', data=X/3.)
					frame.create_dataset('BulkPi', data=X*0.)
					prefactor = 1.0/15.62687/5.068**3
					frame.create_dataset('Temp', data=(X*prefactor)**0.25)
					s = (X + frame['P'].value)/(frame['Temp'].value+1e-14)
					frame.create_dataset('s', data=s)
				if fmt == 'Pi{}{}':
					frame.create_dataset(fmt.format(*a), data=X)
		pi33 = -(frame['Pi00'].value + frame['Pi11'].value \
									 + frame['Pi22'].value)
		frame.create_dataset('Pi33', data=pi33)
		pi3Z = np.zeros_like(pi33)
		frame.create_dataset('Pi03', data=pi3Z)
		frame.create_dataset('Pi13', data=pi3Z)
		frame.create_dataset('Pi23', data=pi3Z)
	f.close()


def main():
	# parse config file
	if len(sys.argv) == 2:
		with open(sys.argv[1], 'r') as f:
			config = dict(
				(i.strip() for i in l.split('=', maxsplit=1))
				for l in f if l[0] != '#'
			)
	else:
		config = {}
	nevents = int(config.get('nevents', 1))
	grid_step = 0.1
	grid_max = 15.05
	dtau = .25*grid_step
	Nhalf = int(grid_max/grid_step)
	proj, targ = 'Pb', 'Pb'
	system = proj+targ
	# run trento and yield initial entropy density arrays
	def initial_conditions(initial_file='initial.hdf'):
		run_cmd(
			'trento {} {}'.format(proj, targ), str(nevents),
			'--grid-step {} --grid-max {}'.format(grid_step, grid_max),
			'--output', initial_file,
			config.get('trento_args', '')
		)
		with h5py.File(initial_file, 'r') as f:
			for dset in f.values():
				yield [dset['matter_density'].value, dset['Ncoll_density'].value]

	# energy
	Exps = ['CMS', 'ALICE']
	sqrts = int(config.get('sqrts'))
	print (proj, targ, '@', sqrts, " GeV")
	# read free streaming time
	tau_fs = float(config.get('tau_fs'))
	xi_fs = float(config.get('xi_fs'))
	# create sampler HRG object (to be reused for all events)
	Tswitch = float(config.get('Tswitch'))
	hrg = frzout.HRG(Tswitch, species='urqmd', res_width=True)
	eswitch = hrg.energy_density()
	NPythiaEvents = int(config.get('NPythiaEvents'))
	# append switching energy density to vishnew arguments
	vishnew_args = [
		config.get('hydro-args', ''),
		'edec={}'.format(eswitch)
	]

	# Heavy quark species (name, ID)
	# mass = {'c': 1.3, 'b': 4.2}
	HM_species = { 'c':
		[('D+', 411), ('D0', 421),
		 ('D*+', 10411), ('D0*', 10421),
		 ('Ds+', 431), ('Ds*+', 433)],
		'b':
		[('B0', 511), ('B+',521),
		 ('B*0', 513), ('B*+', 523),
		 ('Bs0', 531), ('Bs*0',10531)] }
	c_hadron_pid = [s[1] for s in HM_species['c']]
	b_hadron_pid = [s[1] for s in HM_species['b']]

	# run each event
	fresult = h5py.File('results.hdf5','a')
	for ievent, ic in enumerate(initial_conditions()):
		print("# event ", ievent)
		event_gp = fresult.create_group('event_{}'.format(ievent))
		event_gp.attrs.create('initial_entropy', grid_step**2 * ic[0].sum())
		event_gp.attrs.create('N_coll', grid_step**2 * ic[1].sum())

		# ==================Pythia==================================
		run_cmd('pythia-gun', './hic-hvq/share/pythia-setting.txt', str(NPythiaEvents))
		total_weight, sigma_gen, Neve, Nccbar = np.loadtxt("PythiaSummary.dat").T
		if Neve != NPythiaEvents:
			logging.warning("Neve != Neve from the input")
		charm_cross_section_normalization = .5 * sigma_gen / total_weight;
		# ==================IC+freestream===========================
		# save multi frame of freestreaming energy density
		# from xi*tau_fs to tau_fs, with 0.1 < xi < 1.0
		# free stream initial condition
		save_fs_history(ic[0], event_size=grid_max, grid_step=grid_step,
						tau_fs=tau_fs, xi=xi_fs, steps=10, grid_max=grid_max, coarse=4)
		fs = freestream.FreeStreamer(ic[0], grid_max, tau_fs)
		e = fs.energy_density()
		e_above = e[e > eswitch].sum()
		event_gp.attrs.create('mult_factor', e.sum()/e_above if e_above > 0 else 1)
		e.tofile('ed.dat')
		for i in [1, 2]:
			fs.flow_velocity(i).tofile('u{}.dat'.format(i))
		for ij in [(1, 1), (1, 2), (2, 2)]:
			fs.shear_tensor(*ij).tofile('pi{}{}.dat'.format(*ij))

		# ==================Vishnew===========================
		# hydro
		run_cmd('osu-hydro',
				'initialuread=1 iein=0',
				't0={} dt={} dxy={} nls={}'.format(tau_fs, dtau, grid_step, Nhalf),
				*vishnew_args)
		# ==================Frzout Sampler===========================
		# read freeze-out surface data
		surface_data = np.fromfile('surface.dat', dtype='f8').reshape(-1, 16)
		if surface_data.size == 0:
			print("empty event")
			continue
		surface = frzout.Surface(**dict(
			zip(['x', 'sigma', 'v'], np.hsplit(surface_data, [3, 6, 8])),
			pi=dict(zip(['xx', 'xy', 'yy'], surface_data.T[11:14])),
			Pi=surface_data.T[15]),
			ymax=3.)		

		minsamples, maxsamples = 10, 100  # reasonable range for nsamples
		minparts = 30000  # min number of particles to sample
		nparts = 0  # for tracking total number of sampled particles
		# sample soft particles and write to file
		with open('particles_in.dat', 'w') as f:
			nsamples = 0
			while nsamples < maxsamples+1:
				parts = frzout.sample(surface, hrg)
				if parts.size == 0:
					continue
				else:
					nsamples += 1
					nparts += parts.size
					print('#', parts.size, file=f)
					for p in parts:
						print(p['ID'], *chain(p['x'], p['p']), file=f)
					if nparts >= minparts and nsamples >= minsamples:
						break

		event_gp.attrs.create('nsamples', nsamples, dtype=np.int)

		# ==================Heavy Flavor===========================
		run_cmd('run-hvq-events {} {}'.format(sys.argv[1], ievent) )

		# ==================Heavy + Soft --> UrQMD===========================
		run_cmd('run-urqmd {}'.format(nsamples) )

		# ==================Data Processing==================================

		# read final particle data
		ID, charge, fmass, px, py, pz, y, eta, pT0, y0, w, _ = (
			np.array(col, dtype=dtype) for (col, dtype) in
			zip(
				zip(*read_text_file('particles_out.dat')),
				(2*[int] + 10*[float])
			)
		)

		# pT, phi, and id cut
		pT = np.sqrt(px**2+py**2)
		phi = np.arctan2(py, px)
		charged = (charge != 0)
		abs_eta = np.fabs(eta)
		abs_ID = np.abs(ID)
		# It may be redunant to find b-hadron at this stage since UrQMD has
		# not included them yet
		is_c_hadron = np.array([u in c_hadron_pid for u in abs_ID], dtype=bool)
		is_b_hadron = np.array([u in b_hadron_pid for u in abs_ID], dtype=bool)
		is_heavy = np.logical_or(is_c_hadron, is_b_hadron)
		is_light = np.logical_not(is_heavy)

		#============for soft particles======================
		event_gp.attrs.create('dNch_deta', np.count_nonzero(charged & (abs_eta<.5) & is_light) / nsamples)
		for Collab in Exps:
			ref_gp = event_gp.create_group(Collab+"/REF")
			poi_gp = event_gp.create_group(Collab+"/POI")

			#=========Event plane Q-vector from UrQMD events======================
			phi_light = phi[charged & is_light \
				& (JEC[Collab]['vn_ref']['ybins'][0] < eta) \
				& (eta < JEC[Collab]['vn_ref']['ybins'][1]) \
				& (JEC[Collab]['vn_ref']['pTbins'][0] < pT) \
				& (pT < JEC[Collab]['vn_ref']['pTbins'][1])]
			ref_gp.create_dataset('M', data=phi_light.shape[0], dtype=np.int)
			ref_gp.create_dataset('Qn', data=\
				np.array([np.exp(1j*n*phi_light).sum() for n in range(1, 7)]),
					dtype=np.complex)

			#============higher accuracy event-plane=============
			# oversample to get a high percision event plane at freezeout
			ophi_light = np.empty(0)
			nloop=0
			while ophi_light.size < 10**6 and nloop < 100000:
				nloop += 1
				oE, opx, opy, opz = frzout.sample(surface, hrg)['p'].T
				oM, opT, oy, ophi = JLP.fourvec_to_curvelinear(opx, opy, opz, oE)
				ophi = ophi[(JEC[Collab]['vn_ref']['ybins'][0] < oy) \
						& (oy < JEC[Collab]['vn_ref']['ybins'][1]) \
					 	& (JEC[Collab]['vn_ref']['pTbins'][0] < opT) \
						& (opT < JEC[Collab]['vn_ref']['pTbins'][1])]
				ophi_light = np.append(ophi_light, ophi)
			ref_gp.create_dataset('hp-M', data=ophi_light.shape[0], dtype=np.int)
			ref_gp.create_dataset('hp-Qn', data=np.array([np.exp(1j*n*ophi_light).sum()
								for n in range(1, 7)]), dtype=np.complex)
			del ophi_light
			#===========For heavy particles======================
			# For charmed hadrons, use info after urqmd
			HF_dict = { 'pid': abs_ID[is_c_hadron],
				'pT' : pT[is_c_hadron],
				'y'  : y[is_c_hadron],
				'phi': phi[is_c_hadron],
				'w' : w[is_c_hadron]*charm_cross_section_normalization # normalized to an area units
			  	}
			POI = [411, 421, 10411]
			flow = JLP.Qvector(HF_dict, JEC['pred-pT'],#JEC[Collab]['vn_HF']['pTbins'],
						JEC[Collab]['vn_HF']['ybins'], POI)
			Yield = JLP.Yield(HF_dict, JEC['pred-pT'],#JEC[Collab]['Raa']['pTbins'],
						JEC[Collab]['Raa']['ybins'], POI)
			gp = poi_gp.create_group("EPS09")
			for iid in POI:
				name = str(iid)
				gp.create_group('Yield/'+name)
				gp.create_group('vn/'+name)
				gp['Yield/'+name].create_dataset('shape', data=Yield[iid]['shape'])
				gp['Yield/'+name].create_dataset('Ntot', data=Yield[iid]['Ntot'])
				gp['vn/'+name].create_dataset('Qn', data=flow[iid]['Qn'])
				gp['vn/'+name].create_dataset('M', data=flow[iid]['M'])

		# For bottom hadrons, use info at freezeout, since UrQMD does not
		# includes bottom hadrons at this time
		"""
		print("doing B hadron observables")
		for Collab in Exps:
			poi_gp = event_gp.require_group(Collab+"/POI")
			with h5py.File('HeavyAtFrzout.hdf5', 'r') as fh5:
				ds = fh5["event/b/hadron"]
				pid = np.abs(ds['pid'].value).astype(int)
				px, py, pz, E = ds['p'].value.T
				m, pT, y, phi = JLP.fourvec_to_curvelinear(px, py, pz, E)
				w = ds['w'].value
			HF_dict = { 'pid': pid,
				'pT' : pT,
				'y'  : y,
				'phi': phi,
				'w' : w
				  }
			POI = [511, 521, 513, 523]
			flow = JLP.Qvector(HF_dict, JEC['pred-pT'],#JEC[Collab]['vn_B']['pTbins'],
					JEC[Collab]['vn_B']['ybins'], POI)
			Yield = JLP.Yield(HF_dict, JEC['pred-pT'],#JEC[Collab]['Raa_B']['pTbins'],
					JEC[Collab]['Raa_B']['ybins'], POI)
			gp = poi_gp.require_group("EPS09")
			for iid in POI:
				name = str(iid)
				gp.require_group('Yield/'+name)
				gp.require_group('vn/'+name)
				gp['Yield/'+name].create_dataset('shape', data=Yield[iid]['shape'])
				gp['Yield/'+name].create_dataset('Ntot', data=Yield[iid]['Ntot'])
				gp['vn/'+name].create_dataset('Qn', data=flow[iid]['Qn'])
				gp['vn/'+name].create_dataset('M', data=flow[iid]['M'])
		"""
	fresult.close()


if __name__ == "__main__":
	main()
