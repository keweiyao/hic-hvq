#!/bin/bash

# gather system data
uname -nr

# read arguments
inputfile=$1
eventperjob=5
desturl=$2

# load necessary modules
source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash
module load python/3.5.2 all-pkgs gcc/6.2.0 boost/1.62.0 hdf5/1.8.13 gsl/2.3 || exit 1

# unpack package
export pkgname='hic-hvq'
tar xzf $pkgname.tar.gz

# set environment variables
prefix="$(pwd)/$pkgname"
export PATH="$prefix/bin:$PATH"
export XDG_DATA_HOME="$prefix/share"
export PYTHONPATH="$(echo $prefix/lib/python*/site-packages)"
echo $PYTHONPATH
# go!
for i in `seq 1 $eventperjob`; do
  # first clean the info from last run
  if [[ -f initial.hdf ]]; then
    rm initial.hdf
  fi
  if [[ -f JetData.h5 ]]; then
    rm JetData.h5
  fi
  # first, run medium
  run-medium-events $inputfile $i || exit 1
  # second, run hvq event 
  run-hvq-events ./JetData.h5 ./initial.hdf $inputfile $i || exit 1
done

# transfer results
# try to be fault-tolerant
globus() {
  for f in ./*_[0-9]*.dat; do
     globus-url-copy \
    -verbose -create-dest -restart -stall-timeout 30 $@ \
    $f $desturl/soft/$f   
  done
  globus-url-copy \
    -verbose -create-dest -restart -stall-timeout 30 $@ \
    HeavyFlavorResult.hdf5 $desturl/HF_result.hdf5
  
  globus-url-copy \
  -verbose -create-dest -restart -stall-timeout 30 $@ \
  event_weight.txt $desturl/event_weight.dat
}

for i in {1..5}; do
  globus && break
  sleep 5 && false
done || \
  globus -no-data-channel-authentication -no-third-party-transfers
